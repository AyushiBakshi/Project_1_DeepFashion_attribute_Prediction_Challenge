{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AyushiWithDropout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiuxpVS8FWOu"
      },
      "source": [
        "!unzip /content/FashionDataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTFgIcXt-kKt"
      },
      "source": [
        "!pip install d2l\n",
        "!pip install kornia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gts3tOF1-ccg"
      },
      "source": [
        "import os\n",
        "from d2l.torch import argmax, reduce_sum, astype\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
        "import torchvision.datasets as dsets\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage import transform, io\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import random\n",
        "import math\n",
        "from torchvision import models, utils\n",
        "from PIL import Image\n",
        "import datetime\n",
        "#from kornia.losses import focal_loss\n",
        "#import adabound\n",
        "\n",
        "#configs\n",
        "batch_size = 50\n",
        "best_loss = 999\n",
        "best_accu = 0\n",
        "\n",
        "#models\n",
        "dropout1 = 0.2\n",
        "dropout2 = 0.5\n",
        "\n",
        "#image transforms related\n",
        "pretrained_size = 224\n",
        "pretrained_means = [0.485, 0.456, 0.406]\n",
        "pretrained_stds= [0.229, 0.224, 0.225]\n",
        "\n",
        "#Paths for Images\n",
        "\n",
        "PATH = '/content/drive/MyDrive/Resnets/resnet50_22.pt'\n",
        "BEST_LOSS_MODEL_PATH = '/content/drive/MyDrive/Resnets/resnet50_best_22.pt'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LOGS_PATH = '/content/drive/MyDrive/Resnets'\n",
        "IMAGE_NAME = 'resnet_50_22.png'\n",
        "BEST_ACCU_MODEL_PATH = '/content/drive/MyDrive/Resnets/resnet50_best_accu_22.pt'\n",
        "\n",
        "# Data Loaders\n",
        "class DataReader(Dataset):\n",
        "    '''Fashion Dataset'''\n",
        "\n",
        "    def __init__(self, labels_file, root_dir, image_file_ref, bbox_file, transform):\n",
        "\n",
        "        self.data_labels = np.loadtxt(labels_file, dtype=np.uint8)\n",
        "        self.root_dir = root_dir\n",
        "        self.image_file_reference = pd.read_csv(image_file_ref, header=None)\n",
        "        self.bboxes = np.loadtxt(bbox_file, usecols=(0,1,2,3))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_bbox = self.bboxes[idx]\n",
        "        x1 = max(0, int(img_bbox[0]) - 10)\n",
        "        y1 = max(0, int(img_bbox[1]) - 10)\n",
        "        x2 = int(img_bbox[2]) + 10\n",
        "        y2 = int(img_bbox[3]) + 10\n",
        "        bbox_w = x2-x1\n",
        "        bbox_h = y2-y1\n",
        "\n",
        "        img_name = os.path.join(self.root_dir, self.image_file_reference.iloc[idx, 0] )\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        image = image.crop(box=(x1,y1,x2,y2))\n",
        "        image.thumbnail((pretrained_size,pretrained_size), Image.ANTIALIAS)\n",
        "        # image = io.imread(img_name)\n",
        "        labels = torch.from_numpy(self.data_labels[idx])\n",
        "        #bbox = torch.from_numpy(self.bbox_file_ref[idx])\n",
        "        transform_image = self.transform(image)\n",
        "        sample = {'image': transform_image, 'labels': labels}\n",
        "        return sample\n",
        "\n",
        "# Data Transforms\n",
        "data_transform = transforms.Compose([#transforms.ToPILImage(),\n",
        "                                     transforms.RandomChoice(\n",
        "                                         [transforms.Resize((pretrained_size,pretrained_size)),\n",
        "                                         transforms.RandomResizedCrop((pretrained_size,pretrained_size))]\n",
        "                                     ),\n",
        "                                     transforms.RandomPerspective(),\n",
        "                                     transforms.RandomGrayscale(p=0.2),\n",
        "                                     \n",
        "                                     #transforms.CenterCrop(pretrained_size),\n",
        "                                     #transforms.RandomRotation(5),\n",
        "                                     #transforms.Pad(padding=10, fill=0, padding_mode='constant'),\n",
        "                                     #transforms.RandomCrop(pretrained_size),\n",
        "                                     transforms.RandomChoice([\n",
        "                                                              transforms.RandomHorizontalFlip(),\n",
        "                                                              transforms.RandomVerticalFlip()]),\n",
        "                                     #transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize(mean=pretrained_means,\n",
        "                                                        std=pretrained_stds)\n",
        "                                     ])\n",
        "\n",
        "\n",
        "test_data_transform = transforms.Compose([#transforms.ToPILImage(),\n",
        "                                     transforms.Resize(pretrained_size),\n",
        "                                     transforms.CenterCrop(pretrained_size),\n",
        "                                     #transforms.RandomRotation(5),\n",
        "                                     #transforms.RandomResizedCrop(pretrained_size),\n",
        "                                     #transforms.RandomHorizontalFlip(),\n",
        "                                     #transforms.RandomCrop(pretrained_size, padding=10),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize(mean=pretrained_means,\n",
        "                                                        std=pretrained_stds)\n",
        "                                     ])\n",
        "\n",
        "train_dataset = DataReader(\n",
        "    labels_file='/content/FashionDataset/split/train_attr.txt',\n",
        "    root_dir='/content/FashionDataset',\n",
        "    image_file_ref='/content/FashionDataset/split/train.txt',\n",
        "    bbox_file = '/content/FashionDataset/split/train_bbox.txt',\n",
        "    transform=data_transform)\n",
        "\n",
        "test_dataset = DataReader(\n",
        "    labels_file='/content/FashionDataset/split/val_attr.txt',\n",
        "    root_dir='/content/FashionDataset',\n",
        "    image_file_ref='/content/FashionDataset/split/val.txt',\n",
        "    bbox_file = '/content/FashionDataset/split/val_bbox.txt',\n",
        "    transform=test_data_transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True);\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True);\n",
        "\n",
        "\n",
        "# Utilities\n",
        "def show_one_image(tensor_image):\n",
        "    plt.imshow(tensor_image.permute(1, 2, 0))\n",
        "\n",
        "def picshow(tensors):\n",
        "    grid_img = utils.make_grid(tensors, nrow=round(len(tensors)/2))\n",
        "    plt.imshow(grid_img.permute(1, 2, 0))\n",
        "\n",
        "def plot_graph_2_variables(var1, var2, x_label, y_label, legend, title=None, image_name = None):\n",
        "    plt.plot(var1, '-o')\n",
        "    plt.plot(var2, '-o')\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.legend(legend)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    if image_name is None:\n",
        "        #image_name = 'abc.png'\n",
        "        image_name = datetime.datetime.now().strftime(\"%m_%d_%Y_%H%M%S.png\")\n",
        "    fig1 = plt.gcf()\n",
        "    plt.show()\n",
        "    plt.draw()\n",
        "    fig1.savefig(os.path.join(LOGS_PATH,image_name), dpi=100)\n",
        "\n",
        "# ResNet-50 with modified fully connected layer\n",
        "class MyResnetModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        self.fc = self.resnet.fc\n",
        "        self.in_features = self.resnet.fc.in_features\n",
        "        self.feature_extractor = torch.nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
        "        \n",
        "        self.linear_label1 = nn.Linear(2048, out_features=7)\n",
        "        self.linear_label2 = nn.Linear(2048, out_features=3)\n",
        "        self.linear_label3 = nn.Linear(2048, out_features=3)\n",
        "        self.linear_label4 = nn.Linear(2048, out_features=4)\n",
        "        self.linear_label5 = nn.Linear(2048, out_features=6)\n",
        "        self.linear_label6 = nn.Linear(2048, out_features=3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout1)\n",
        "        self.untrained_linear_label = nn.Linear(2048, 2048)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out_features = self.feature_extractor(input)\n",
        "        out_features = out_features.view(out_features.size(0), -1)\n",
        "        out_features = self.relu(self.untrained_linear_label(out_features))\n",
        "        out_features = self.dropout(out_features)\n",
        "        \n",
        "        out_pred_1 = self.linear_label1(out_features)\n",
        "        out_pred_2 = self.linear_label2(out_features)\n",
        "        out_pred_3 = self.linear_label3(out_features)\n",
        "        out_pred_4 = self.linear_label4(out_features)\n",
        "        out_pred_5 = self.linear_label5(out_features)\n",
        "        out_pred_6 = self.linear_label6(out_features)\n",
        "        return out_pred_1, out_pred_2, out_pred_3, out_pred_4, out_pred_5, out_pred_6\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "model = MyResnetModel()\n",
        "print(model)\n",
        "model.apply(init_weights)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(device)\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "\n",
        "def show_images(images, cols=1, titles=None):\n",
        "    \"\"\"Display a list of images in a single figure with matplotlib.\n",
        "\n",
        "    Parameters\n",
        "    ---------\n",
        "    images: List of np.arrays compatible with plt.imshow.\n",
        "\n",
        "    cols (Default = 1): Number of columns in figure (number of rows is\n",
        "                        set to np.ceil(n_images/float(cols))).\n",
        "\n",
        "    titles: List of titles corresponding to each image. Must have\n",
        "            the same length as titles.\n",
        "    \"\"\"\n",
        "    assert ((titles is None) or (len(images) == len(titles)))\n",
        "    n_images = len(images)\n",
        "    if titles is None: titles = ['Image (%d)' % i for i in range(1, n_images + 1)]\n",
        "    fig = plt.figure()\n",
        "    for n, (image, title) in enumerate(zip(images, titles)):\n",
        "        a = fig.add_subplot(cols, np.ceil(n_images / float(cols)), n + 1)\n",
        "        if image.ndim == 2:\n",
        "            plt.gray()\n",
        "        plt.imshow(image)\n",
        "        a.set_title(title)\n",
        "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
        "    plt.show()\n",
        "\n",
        "def get_accuracy(pred_cat_1, pred_cat_2, pred_cat_3, pred_cat_4, pred_cat_5, pred_cat_6, target_label):\n",
        "    pred_cat_1 = argmax(pred_cat_1, axis=1)\n",
        "    pred_cat_2 = argmax(pred_cat_2, axis=1)\n",
        "    pred_cat_3 = argmax(pred_cat_3, axis=1)\n",
        "    pred_cat_4 = argmax(pred_cat_4, axis=1)\n",
        "    pred_cat_5 = argmax(pred_cat_5, axis=1)\n",
        "    pred_cat_6 = argmax(pred_cat_6, axis=1)\n",
        "\n",
        "    pred_cat_1 = pred_cat_1.reshape([pred_cat_1.shape[0],1])\n",
        "    pred_cat_2 = pred_cat_2.reshape([pred_cat_2.shape[0],1])\n",
        "    pred_cat_3 = pred_cat_3.reshape([pred_cat_3.shape[0],1])\n",
        "    pred_cat_4 = pred_cat_4.reshape([pred_cat_4.shape[0],1])\n",
        "    pred_cat_5 = pred_cat_5.reshape([pred_cat_5.shape[0],1])\n",
        "    pred_cat_6 = pred_cat_6.reshape([pred_cat_6.shape[0],1])\n",
        "\n",
        "    pred = torch.cat((pred_cat_1, pred_cat_2, pred_cat_3, pred_cat_4, pred_cat_5, pred_cat_6), 1)\n",
        "\n",
        "    comparison = astype(pred, target_label.dtype) == target_label\n",
        "    return float(reduce_sum(astype(comparison, target_label.dtype)))\n",
        "\n",
        "\n",
        "steps = 0\n",
        "\n",
        "learning_rate = 0.05;\n",
        "epochs = 40\n",
        "\n",
        "# Training and Evaluation\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#riterion = focal_loss()\n",
        "#optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "#optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.05, weight_decay=0.01) #momentum=0.1)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', patience= 3, factor=0.5, verbose=True )\n",
        "#scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "train_loss_arr = []\n",
        "eval_loss_arr = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss, valid_loss = 0.0, 0.0\n",
        "    steps = 0\n",
        "    saved = False\n",
        "\n",
        "    for tr_data in train_loader:\n",
        "        inputs = tr_data['image']\n",
        "        labels = tr_data['labels']\n",
        "        steps += 1\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        pc1, pc2, pc3, pc4, pc5, pc6 = model(inputs)\n",
        "\n",
        "        # loss1 = focal_loss(pc1.to(device), (labels[:, 0].type(torch.LongTensor)).to(device),alpha=0.5, gamma=2.0, reduction='mean')\n",
        "        # loss2 = focal_loss(pc2.to(device), (labels[:, 1].type(torch.LongTensor)).to(device),alpha=0.5, gamma=2.0, reduction='mean')\n",
        "        # loss3 = focal_loss(pc3.to(device), (labels[:, 2].type(torch.LongTensor)).to(device),alpha=0.5, gamma=2.0, reduction='mean')\n",
        "        # loss4 = focal_loss(pc4.to(device), (labels[:, 3].type(torch.LongTensor)).to(device),alpha=0.5, gamma=2.0, reduction='mean')\n",
        "        # loss5 = focal_loss(pc5.to(device), (labels[:, 4].type(torch.LongTensor)).to(device),alpha=0.5, gamma=2.0, reduction='mean')\n",
        "        # loss6 = focal_loss(pc6.to(device), (labels[:, 5].type(torch.LongTensor)).to(device),alpha=0.5, gamma=2.0, reduction='mean')\n",
        "        loss1 = criterion(pc1.to(device), (labels[:, 0].type(torch.LongTensor)).to(device))\n",
        "        loss2 = criterion(pc2.to(device), (labels[:, 1].type(torch.LongTensor)).to(device))\n",
        "        loss3 = criterion(pc3.to(device), (labels[:, 2].type(torch.LongTensor)).to(device))\n",
        "        loss4 = criterion(pc4.to(device), (labels[:, 3].type(torch.LongTensor)).to(device))\n",
        "        loss5 = criterion(pc5.to(device), (labels[:, 4].type(torch.LongTensor)).to(device))\n",
        "        loss6 = criterion(pc6.to(device), (labels[:, 5].type(torch.LongTensor)).to(device))\n",
        "        sum_loss = loss1 + loss2 + loss3 + loss4 + loss5 + loss6\n",
        "        sum_loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += sum_loss.item()\n",
        "        print('Iter : %s/%s,  Loss: %s' % (steps, len(train_loader), sum_loss.item()))\n",
        "    tr_loss = train_loss / len(train_loader)\n",
        "    train_loss_arr.append(tr_loss)\n",
        "    model.eval()\n",
        "    sum_accuracy = 0\n",
        "    vl_loss = 0\n",
        "    total_accuracy = 0\n",
        "    with torch.no_grad():\n",
        "        val_steps = 0\n",
        "        for tst_dt in test_loader:\n",
        "            tst_loader_len = len(test_loader)\n",
        "            inputs = tst_dt['image']\n",
        "            labels = tst_dt['labels']\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            val_steps += 1\n",
        "            pc1, pc2, pc3, pc4, pc5, pc6 = model(inputs)\n",
        "            # loss1 = focal_loss(pc1.to(device), (labels[:, 0].type(torch.LongTensor)).to(device),alpha=0.5, gamma=2.0, reduction='mean')\n",
        "            # loss2 = focal_loss(pc2.to(device), (labels[:, 1].type(torch.LongTensor)).to(device),alpha=0.5, gamma=2.0, reduction='mean')\n",
        "            # loss3 = focal_loss(pc3.to(device), (labels[:, 2].type(torch.LongTensor)).to(device),alpha=0.5, gamma=2.0, reduction='mean')\n",
        "            # loss4 = focal_loss(pc4.to(device), (labels[:, 3].type(torch.LongTensor)).to(device),alpha=0.5, gamma=2.0, reduction='mean')\n",
        "            # loss5 = focal_loss(pc5.to(device), (labels[:, 4].type(torch.LongTensor)).to(device),alpha=0.5, gamma=2.0, reduction='mean')\n",
        "            # loss6 = focal_loss(pc6.to(device), (labels[:, 5].type(torch.LongTensor)).to(device),alpha=0.5, gamma=2.0, reduction='mean')\n",
        "            loss1 = criterion(pc1.to(device), (labels[:, 0].type(torch.LongTensor)).to(device))\n",
        "            loss2 = criterion(pc2.to(device), (labels[:, 1].type(torch.LongTensor)).to(device))\n",
        "            loss3 = criterion(pc3.to(device), (labels[:, 2].type(torch.LongTensor)).to(device))\n",
        "            loss4 = criterion(pc4.to(device), (labels[:, 3].type(torch.LongTensor)).to(device))\n",
        "            loss5 = criterion(pc5.to(device), (labels[:, 4].type(torch.LongTensor)).to(device))\n",
        "            loss6 = criterion(pc6.to(device), (labels[:, 5].type(torch.LongTensor)).to(device))\n",
        "            sum_loss = loss1 + loss2 + loss3 + loss4 + loss5 + loss6\n",
        "            valid_loss += sum_loss.item()\n",
        "\n",
        "            accuracy_float = get_accuracy((pc1).to(device),\n",
        "                                          (pc2).to(device),\n",
        "                                          (pc3).to(device),\n",
        "                                          (pc4).to(device),\n",
        "                                          (pc5).to(device),\n",
        "                                          (pc6).to(device), (labels).to(device))\n",
        "            accuracy = accuracy_float / (batch_size * 6)\n",
        "            total_accuracy += accuracy\n",
        "\n",
        "            print(\n",
        "                'Iter : %s/%s,  Accuracy: %s' % (val_steps, tst_loader_len, accuracy))\n",
        "\n",
        "    curr_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    vl_loss = valid_loss / tst_loader_len\n",
        "    eval_loss_arr.append(vl_loss)\n",
        "    val_accuracy = total_accuracy/tst_loader_len\n",
        "    print('Epoch : %s, Training Loss: %s,  Validation Loss: %s, Validation Accuracy: %s, LR: %s , Time: %s' % (epoch + 1, tr_loss, vl_loss, val_accuracy, curr_lr, datetime.datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\")))\n",
        "    #scheduler.step()\n",
        "    scheduler.step(vl_loss)\n",
        "    # Save best model\n",
        "    if vl_loss < best_loss  :\n",
        "        print('saving best loss model at time : %s ' % datetime.datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
        "        best_loss = vl_loss\n",
        "        saved = True\n",
        "        torch.save(model.state_dict(), BEST_LOSS_MODEL_PATH)\n",
        "    \n",
        "    if val_accuracy > best_accu :\n",
        "        best_accu = val_accuracy        \n",
        "        saved = True\n",
        "        print('saving best accuracy model at time : %s ' % datetime.datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
        "        torch.save(model.state_dict(), BEST_ACCU_MODEL_PATH)\n",
        "      \n",
        "    if curr_lr <= 0.00000005:\n",
        "      print('Terminating because lr has gone too low')\n",
        "      break\n",
        "        \n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "plot_graph_2_variables(train_loss_arr,eval_loss_arr,'epoch','losses',['Train','Valid'], 'Train vs Valid Losses', IMAGE_NAME)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "NOg4TPyPzYAM",
        "outputId": "70cd1866-b413-4755-d27e-e8e23c46abb1"
      },
      "source": [
        "import os\n",
        "from d2l.torch import argmax, reduce_sum, astype\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
        "import torchvision.datasets as dsets\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage import transform, io\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import random\n",
        "import math\n",
        "from torchvision import models, utils\n",
        "from PIL import Image\n",
        "import datetime\n",
        "def plot_graph_2_variables(var1, var2, x_label, y_label, legend, title=None, image_name = None):\n",
        "    plt.plot(var1, '-o')\n",
        "    plt.plot(var2, '-o')\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.legend(legend)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    if image_name is None:\n",
        "        #image_name = 'abc.png'\n",
        "        image_name = datetime.datetime.now().strftime(\"%m_%d_%Y_%H%M%S.png\")\n",
        "    fig1 = plt.gcf()\n",
        "    plt.show()\n",
        "    plt.draw()\n",
        "    fig1.savefig(os.path.join(LOGS_PATH,image_name), dpi=100)\n",
        "plot_graph_2_variables([7.50, 6.07,6.03,6.01,5.99,5.96 ],[6.11,6.10,6.41,5.94,5.98,6.05],'epoch','losses',['Train','Valid'], 'Train vs Valid Losses')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7cc1f3f34f22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mfig1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOGS_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mplot_graph_2_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7.50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.07\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6.03\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5.96\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6.11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6.10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6.41\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5.94\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5.98\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6.05\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'losses'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train vs Valid Losses'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-7cc1f3f34f22>\u001b[0m in \u001b[0;36mplot_graph_2_variables\u001b[0;34m(var1, var2, x_label, y_label, legend, title, image_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mfig1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOGS_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mplot_graph_2_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7.50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.07\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6.03\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5.96\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6.11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6.10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6.41\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5.94\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5.98\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6.05\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'losses'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train vs Valid Losses'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LOGS_PATH' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vmu9oJbMamC",
        "outputId": "b6223af6-a5e5-4367-ccad-d9dd3fc7b657"
      },
      "source": [
        "import os\n",
        "import time\n",
        "BEST_ACCU_MODEL_PATH = '/content/drive/MyDrive/Resnets/resnet50_best_accu_13.pt'  \n",
        "path = BEST_ACCU_MODEL_PATH\n",
        "  \n",
        "ti_m = os.path.getmtime(path)\n",
        "  \n",
        "m_ti = time.ctime(ti_m)\n",
        "  \n",
        "# Using the timestamp string to create a \n",
        "# time object/structure\n",
        "t_obj = time.strptime(m_ti)\n",
        "  \n",
        "# Transforming the time object to a timestamp \n",
        "# of ISO 8601 format\n",
        "T_stamp = time.strftime(\"%Y-%m-%d %H:%M:%S\", t_obj)\n",
        "  \n",
        "print(f\"The file located at the path {path} was last modified at {T_stamp}\")\n",
        "\n",
        "import datetime\n",
        "print(datetime.datetime.now())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file located at the path /content/drive/MyDrive/Resnets/resnet50_best_accu_13.pt was last modified at 2021-10-12 11:44:52\n",
            "2021-10-12 12:25:29.557799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv1iie5RNlNv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c52dcd-788f-4853-d1fa-6b182b8f8bee"
      },
      "source": [
        "LOAD_PATH =BEST_ACCU_MODEL_PATH\n",
        "\n",
        "\n",
        "import sys\n",
        " \n",
        "file_path = '/content/logs/prediction.txt'\n",
        "sys.stdout = open(file_path, \"w\")\n",
        "\n",
        "# Data Reader\n",
        "class DataReaderForTest(Dataset):\n",
        "    '''Fashion Dataset'''\n",
        "\n",
        "    def __init__(self, root_dir, image_file_ref,bbox_file, transform):\n",
        "        images = open(os.path.join(image_file_ref)).readlines()\n",
        "        self.img_list = [x.strip() for x in images]\n",
        "        self.root_dir = root_dir\n",
        "        self.image_file_reference = pd.read_csv(image_file_ref, header=None)\n",
        "        self.bboxes = np.loadtxt(bbox_file, usecols=(0,1,2,3))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_bbox = self.bboxes[idx]\n",
        "        x1 = max(0, int(img_bbox[0]) - 10)\n",
        "        y1 = max(0, int(img_bbox[1]) - 10)\n",
        "        x2 = int(img_bbox[2]) + 10\n",
        "        y2 = int(img_bbox[3]) + 10\n",
        "        bbox_w = x2-x1\n",
        "        bbox_h = y2-y1\n",
        "        img_name = self.root_dir+'/'+self.image_file_reference.iloc[idx, 0]\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        image = image.crop(box=(x1,y1,x2,y2))\n",
        "        image.thumbnail((pretrained_size,pretrained_size), Image.ANTIALIAS)\n",
        "\n",
        "        transform_image = self.transform(image)\n",
        "        sample = {'image': transform_image}\n",
        "        return sample\n",
        "\n",
        "# Data Loader\n",
        "data_transform = test_data_transform\n",
        "\n",
        "# data_transform = transforms.Compose([#transforms.ToPILImage(),\n",
        "#                                      transforms.Resize(pretrained_size),\n",
        "#                                      transforms.CenterCrop(pretrained_size),\n",
        "#                                      #transforms.RandomRotation(5),\n",
        "#                                      #transforms.RandomResizedCrop(pretrained_size),\n",
        "#                                      #transforms.RandomHorizontalFlip(),\n",
        "#                                      #transforms.RandomCrop(pretrained_size, padding=10),\n",
        "#                                      transforms.ToTensor(),\n",
        "#                                      transforms.Normalize(mean=pretrained_means,\n",
        "#                                                         std=pretrained_stds)\n",
        "#                                      ])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_dataset = DataReaderForTest(\n",
        "    root_dir='/content/FashionDataset',\n",
        "    image_file_ref='/content/FashionDataset/split/test.txt',\n",
        "    bbox_file ='/content/FashionDataset/split/test_bbox.txt' ,\n",
        "    transform=data_transform)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False);\n",
        "device = torch.device('cpu')\n",
        "model = MyResnetModel()\n",
        "model.load_state_dict(torch.load(LOAD_PATH, map_location='cpu'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def get_label_prediction(pred_cat_1, pred_cat_2, pred_cat_3, pred_cat_4, pred_cat_5, pred_cat_6):\n",
        "    pred_cat_1 = argmax(pred_cat_1, axis=1)\n",
        "    pred_cat_2 = argmax(pred_cat_2, axis=1)\n",
        "    pred_cat_3 = argmax(pred_cat_3, axis=1)\n",
        "    pred_cat_4 = argmax(pred_cat_4, axis=1)\n",
        "    pred_cat_5 = argmax(pred_cat_5, axis=1)\n",
        "    pred_cat_6 = argmax(pred_cat_6, axis=1)\n",
        "\n",
        "    pred_cat_1 = pred_cat_1.reshape([pred_cat_1.shape[0],1])\n",
        "    pred_cat_2 = pred_cat_2.reshape([pred_cat_2.shape[0],1])\n",
        "    pred_cat_3 = pred_cat_3.reshape([pred_cat_3.shape[0],1])\n",
        "    pred_cat_4 = pred_cat_4.reshape([pred_cat_4.shape[0],1])\n",
        "    pred_cat_5 = pred_cat_5.reshape([pred_cat_5.shape[0],1])\n",
        "    pred_cat_6 = pred_cat_6.reshape([pred_cat_6.shape[0],1])\n",
        "\n",
        "    pred = torch.cat((pred_cat_1, pred_cat_2, pred_cat_3, pred_cat_4, pred_cat_5, pred_cat_6), 1)\n",
        "    #print(pred)\n",
        "    for x in pred:\n",
        "      print('%s %s %s %s %s %s' % ( x[0].item(),x[1].item(),x[2].item(),x[3].item(),x[4].item(),x[5].item() )) \n",
        "      \n",
        "\n",
        "with torch.no_grad():\n",
        "    val_steps = 0\n",
        "    for tst_dt in test_loader:\n",
        "        tst_loader_len = len(test_loader)\n",
        "        inputs = tst_dt['image']\n",
        "        inputs = inputs.to(device)\n",
        "        val_steps += 1\n",
        "        pc1, pc2, pc3, pc4, pc5, pc6 = model(inputs)\n",
        "        get_label_prediction((pc1).to(device),\n",
        "                             (pc2).to(device),\n",
        "                             (pc3).to(device),\n",
        "                             (pc4).to(device),\n",
        "                             (pc5).to(device),\n",
        "                             (pc6).to(device))\n",
        "        #break\n",
        "       # print(\n",
        "        #    'Iter : %s/%s ' % (val_steps, len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uIIo_jvOA2Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}